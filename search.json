[
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "We study the motions of complex molecules, how they are organized, and how they enable chemistry.\n\nMembrane-bound Proteins\nLife must perform the most general of chemistries in order to control the flow of molecules into and out of cells, and to send and receive information out into the environment. Somehow membrane-bound proteins have evolved to peform that chemistry, and to do it without making mistakes. How do these nanoscopic “machines” perform their tasks in the face of a tremendous amount of interference from their environment? We study the intrinsic fluctuations of membrane-bound proteins and other molecules in order to understand how they function, and how they are regulated in response to the selective pressure of survival.\n\n\n\nNanoscopic control of chemicals and information.\n\n\n\n\nSingle-Molecule Techniques\nTraditionally, chemists analyze collections of so many molecules at once that they can only observe the average behavior. Imagine going to an art gallery to enjoy all of the paintings, only to instead see one mixture of all of the paintings together. Using single-molecule techniques, such as fluorescence resonance energy transfer (FRET) microscopy, we can routinely observe individual molecules as they work. These powerful approaches are technically demanding, and difficult to analyze, but provide unparalled resolution into the molecular processes and mechanisms behind life.\n\n\n\nChemistry, one molecule at a time.\n\n\n\n\nBayesian Inference\nThe data collected from individual molecules is noisy, and is only a low-dimensional representation of the entire molecule. Despite these shortcomings, in single-molecule experiments, we must infer the underlying molecular details from this data, and use them to understand the overall energy landscape of the molecule. We are experts at developing and using Bayesian inference and machine learning algorithms to perform those analyses, and in a way that enables previously impossible experiments.\n\n\n\nComputational insight into the molecular scale"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "Our lab is located in Olson Hall at Rutgers University-Newark in downtown Newark, NJ.\n73 Warren St, Newark, NJ 07102"
  },
  {
    "objectID": "posts/2023-10-29-median.html",
    "href": "posts/2023-10-29-median.html",
    "title": "Fast Median Filters",
    "section": "",
    "text": "Introduction\nA median filter is a non-linear operation that replaces the value of a pixel by the median value of it’s neighbors. It can be very useful for removing salt and pepper noise from images. It’s not uncommon to see “salt” when a cosmic ray hits your camera detector (e.g., a pixel on an sCMOS chip). That type of noise will really mess up any downstream processing algorithms, because they usually don’t assume the presence of such noise. Unfortunately, median filters are very slow, so it’s not always practical to use them in a data analysis pipeline. So, I’ve done a little digging into how to speed them up for my Python code.\n\n\nLiterature\nThis follows the following two works:\n\nHuang, T.S., Yang, G.J., Tang, G.Y. A Fast Two-Dimensional Median Filtering Algorithm. IEEE Transactions on Acoustics, Speech, and Signal Processing, 27(1), 1979.\nPerreault, S., Hebert, P. Median Filtering in Constant Time. IEEE Transactions on Image Processing, 16, 2389–2394, 2007.\n\nn.b., the Perreault manuscript reviews the Huang median filter.\n\n\nCode\nHere’s some non-optimized code to perform both the Huang and Perreault median filters on 2D, 8-bit images. Both of these are histogram-based median filters, where the idea is the the median value is found from a histogram instead of using a sorting filter. This means that, here, I’m maintaining an 256-bin histogram and looping through it until the cumulative counts reach 50% of the total counts. Also, w is the width of the kernel window, and must be an odd number. For the edges, they are treated as just having smaller neighborhoods.\n\n\nCode\nimport numpy as np\nimport numba as nb\n\n\n@nb.njit\ndef med_huang_boundary(dd,w):\n    nx,ny = dd.shape\n    filtered = np.zeros_like(dd)\n    histogram = np.zeros(256,dtype='int')\n    w2 = int(w*w)\n    r = int((w-1)//2)\n\n    for i in range(nx):\n\n        ### zero histogram\n        for hi in range(256):\n            histogram[hi] = 0\n        \n        ### initialize histogram\n        j = 0\n        nn = 0\n        for k in range(-r,r+1):\n            for l in range(-r,r+1):\n                hi = i+k\n                hj = j+l\n                if hi >=0 and hj>=0 and hi<nx and hj<ny:\n                    histogram[dd[hi,hj]] += 1\n                    nn += 1\n\n        ## find median of histogram\n        cut = int((nn//2)+1)\n        count = 0\n        for ci in range(histogram.size):\n            count += histogram[ci]\n            if count >= cut:\n                filtered[i,j] = ci\n                break\n\n        ### run row\n        for j in range(1,ny):\n            hjl = j-r -1\n            hjr = j+r\n                \n            for k in range(-r,r+1):\n                hi = i+k\n                ## add RHS histogram\n                if hi >=0 and hi<nx and hjr<ny:\n                    histogram[dd[hi,hjr]] += 1\n                    nn += 1\n                ## remove LHS histogram\n                if hi >=0 and hjl>=0 and hi<nx:\n                    histogram[dd[hi,hjl]] -= 1\n                    nn -= 1\n\n            ## find median of histogram\n            cut = int((nn//2)+1)\n            count = 0\n            for ci in range(histogram.size):\n                count += histogram[ci]\n                if count >= cut:\n                    filtered[i,j] = ci\n                    break\n    return filtered\n\n@nb.njit\ndef med_perreault_boundary(dd,w):\n    nx,ny = dd.shape\n    filtered = np.zeros_like(dd)\n    kernel_histogram = np.zeros(256,dtype='int')\n    nn = 0\n    column_histograms = np.zeros((ny,256),dtype='int')\n    nnc = np.zeros(ny,dtype='int')\n    \n    w2 = int(w*w)\n    r = int((w-1)//2)\n\n    ######### Initialize things\n    i = 0\n    j = 0\n    \n    ##initialize column histograms\n    for j in range(ny):\n        for k in range(r+1):\n            column_histograms[j,dd[k,j]] += 1\n            nnc[j] += 1\n    \n    ## initialize kernel histogram\n    for l in range(r+1):\n        kernel_histogram += column_histograms[l]\n        nn += nnc[l]\n    \n    ### first row doesn't get updates\n    for j in range(ny):\n        if j > 0:\n            hjl = j - r - 1\n            hjr = j + r\n            if hjl >= 0:\n                kernel_histogram -= column_histograms[hjl]\n                nn -= nnc[hjl]\n            if hjr < ny:\n                kernel_histogram += column_histograms[hjr]\n                nn += nnc[hjr]\n            \n        cut = int((nn//2)+1)\n        count = 0\n        \n        for ci in range(kernel_histogram.size):\n            count += kernel_histogram[ci]\n            if count >= cut:\n                filtered[i,j] = ci\n                break\n\n    ######### Do Rows \n    for i in range(1,nx):\n        for j in range(ny):\n            ## start the next row\n            if j == 0:\n                kernel_histogram *= 0\n                nn = 0\n                hit = i-r-1\n                hib = i+r\n                for l in range(r+1):\n                    if hit >= 0:\n                        column_histograms[l,dd[hit,l]] -= 1\n                        nnc[l] -= 1\n                    if hib < nx:\n                        column_histograms[l,dd[hib,l]] += 1\n                        nnc[l] += 1\n                    kernel_histogram += column_histograms[l]\n                    nn += nnc[l]\n            \n            ## go through the row\n            else:\n                hit = i-r-1\n                hib = i+r\n                hjl = j-r-1\n                hjr  = j+r\n\n                #### update column histograms\n                ## top\n                if hit >= 0:\n                    column_histograms[hjr,dd[hit,hjr]] -= 1\n                    nnc[hjr] -= 1\n\n                ## bottom\n                if hib < nx:\n                    column_histograms[hjr,dd[hib,hjr]] += 1\n                    nnc[hjr] += 1\n\n                #### update kernel histogram\n                ## left\n                if hjl >= 0:\n                    kernel_histogram -= column_histograms[hjl]\n                    nn -= nnc[hjl]\n\n                ## right\n                if hjr < ny:\n                    kernel_histogram += column_histograms[hjr]\n                    nn += nnc[hjr]\n                    \n            \n            ## find median of kernel histogram\n            cut = int((nn//2)+1)\n            count = 0\n            for ci in range(kernel_histogram.size):\n                count += kernel_histogram[ci]\n                if count >= cut:\n                    filtered[i,j] = ci\n                    break\n\n    return filtered\n\n\n\n\nTiming\nA sorting-based median filter scales with the size of the kernel window, i.e., \\(\\mathcal{O}(r^2)\\), where \\(2r+1\\) is width of the kernel. Ignoring all overhead, and some issues with the edges, the Huang median filter scales with \\(\\mathcal{O}(r)\\), and the Perreault median filter scales with \\(\\mathcal{O}(1)\\). For Perreault, that means it is constant time WRT to kernel size – obviously, for all of these median filters, the larger the image, the larger the processing time.\nHere’s a quick investigation of the timing\n\n\nCode\n##### Simulate image\nfrom scipy.ndimage import uniform_filter,gaussian_filter\nnx,ny = (1024,1024)\n\n## Make some blurry features\nd = np.random.seed(666)\nd = (np.random.rand(nx,ny)*256).astype('uint8')\nd = uniform_filter(d,8)\nd += (np.random.poisson(d.astype('double')*500)//100).astype('uint8')\nd = gaussian_filter(d,8).astype('uint8')\n\n## Add salt and pepper noise\ndd = d.copy()\ndd[np.random.rand(nx,ny)>.995] = 255\ndd[np.random.rand(nx,ny)>.995] = 0\n\n\n\n\nCode\n#### Plot the Image\nimport matplotlib.pyplot as plt\n\nzoom = 64\nvmin = d[:zoom,:zoom].min()\nvmax = d[:zoom,:zoom].max()\n\nfig,ax = plt.subplots(2,3,figsize=(9,6),dpi=300)\n#### Image\nax[0,0].imshow(dd,cmap='Greys')\nax[0,1].imshow(dd[:zoom,:zoom],cmap='Greys',interpolation='nearest')\nax[0,2].imshow(dd[:zoom,:zoom],cmap='Greys',interpolation='nearest',vmin=vmin,vmax=vmax)\n\n#### Median filtered image\nq = med_huang_boundary(dd,9)\nax[1,0].imshow(q,cmap='Greys')\nax[1,1].imshow(q[:zoom,:zoom],cmap='Greys',interpolation='nearest')\nax[1,2].imshow(q[:zoom,:zoom],cmap='Greys',interpolation='nearest',vmin=vmin,vmax=vmax)\n\nax[0,0].set_title('Full image')\nax[0,1].set_title('Zoomed image')\nax[0,2].set_title('Scaled image')\n\nax[1,0].set_title('Filtered Full image')\nax[1,1].set_title('Filtered Zoomed image')\nax[1,2].set_title('Filtered Scaled image')\n\nfig.tight_layout()\n[[aaa.axis('off') for aaa in aa] for aa in ax]\nplt.show()\n\n\n\n\n\n\n\nCode\nimport time\nfrom tqdm.notebook import tqdm\nfrom scipy.ndimage import median_filter\n\niters = 10\n\nws = np.arange(1,251)*2+1\ntimes = np.zeros((3,ws.size)) + np.nan\n\n#### Huang's median filter\nfor i in tqdm(range(100),desc='Huang'):\n    ts = []\n    for _ in range(iters):\n        t0 = time.time()\n        med_huang_boundary(dd,ws[i])\n        t1 = time.time()\n        ts.append(t1-t0)\n        times[1,i] = np.median(ts)\n    # print(i,ws[i],times[1,i])\n# print('Done Huang')\n\n#### Perreault's median filter\nfor i in tqdm(range(ws.size),desc='Perreault'):\n    ts = []\n    for _ in range(iters):\n        t0 = time.time()\n        med_perreault_boundary(dd,ws[i])\n        t1 = time.time()\n        ts.append(t1-t0)\n        times[2,i] = np.median(ts)\n    # print(i,ws[i],times[2,i])\n# print('Done Perreault')\n\n##### Scipy's median filter (from ndimage)\nfor i in tqdm(range(20),desc='Scipy'):\n    ts = []\n    for _ in range(iters//2):\n        t0 = time.time()\n        median_filter(dd,ws[i])\n        t1 = time.time()\n        ts.append(t1-t0)\n        times[0,i] = np.median(ts)\n    # print(i,ws[i],times[0,i])\n# print('Done Scipy')\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nplt.plot(ws,times[0],label='Scipy, $\\mathcal{O}(r^2)$',lw=1.5,marker='o',alpha=.5,linestyle='none')\nplt.plot(ws,times[1],label='Huang, $\\mathcal{O}(r^1)$',lw=1.5,marker='o',alpha=.5,linestyle='none')\nplt.plot(ws,times[2],label='Perreault, $\\mathcal{O}(r^0)$',lw=1.5,marker='o',alpha=.5,linestyle='none')\n\np0 = np.polyfit(ws[np.isfinite(times[0])],times[0][np.isfinite(times[0])],2)\np1 = np.polyfit(ws[ws<100],times[1][ws<100],1)\nkeep = np.bitwise_and(np.isfinite(times[1]),ws>125)\np1b = np.polyfit(ws[keep],times[1][keep],1)\np2 = np.polyfit(ws,times[2],0)\n\nx = np.logspace(np.log10(3),np.log10(ws[-1]),1000)\ny0 = p0[0]*x*x + p0[1]*x + p0[2]\ny1 = p1[0]*x + p1[1]\ny1b = p1b[0]*x + p1b[1]\ny2 = p2[0] + (x*0)\nplt.plot(x,y0,color='k',lw=1,ls='--',alpha=.7)\nplt.plot(x,y1,color='k',lw=1,ls='--',alpha=.7)\nplt.plot(x[x>125],y1b[x>125],color='k',lw=1,ls='--',alpha=.7)\nplt.plot(x,y2,color='k',lw=1,ls='--',alpha=.7)\n\ndiff = np.square(times[1]-times[2])\nswitch = ws[diff[np.isfinite(diff)].argmin()]\nplt.axvline(x=switch,label='w=%d'%(switch),color='red',linewidth=1,alpha=.8,zorder=-2)\n# plt.axvline(x=125,color='k',linewidth=1,ls='--',alpha=.7)\n\nplt.xlabel('Kernel width')\nplt.ylabel('Average time for a %d x %d image (s)'%(dd.shape[0],dd.shape[1]))\nplt.legend()\nplt.ylim(.03,10)\n# plt.xlim(0,201)\nplt.yscale('log')\nplt.xscale('log')\nplt.show()\n\n\n\n\n\n\n\nConclusions\nAs you can seen in the plot of the timing, this Python code matches the theoretical scaling WRT the kernel size (\\(r^2\\), \\(r^1\\) and \\(r^0\\)). It also replicates the cross-over point between the Huang and Perreault approaches from the Perreault paper. As they explain, this is largely because of the overhead associated with initializing the column histograms in Perreault’s approach.\nOverall, this isn’t very optimized Python code, which is part of why the Scipy filter is the fastest for a 3x3 kernel – however, that quickly becomes very slow.\nI am planing to use these to remove salt and pepper noise for spot finding in single-molcule localization microscopy."
  },
  {
    "objectID": "posts/2017-06-21-maxvalue.html",
    "href": "posts/2017-06-21-maxvalue.html",
    "title": "Estimating Max-value Distributions",
    "section": "",
    "text": "So… for max value distributions, the CDF of seeing the max, \\(M_n = max_i \\{x_1,\\cdots,x_n\\}\\), is\n\\[P(M_n \\leq x) = \\prod_i^n F(x),\\]\nwhere \\(F(x)\\) is the CDF of the distribution from which the max was chosen.\nTherefore to get the PDF, just differentiate with respect to \\(x\\) to get,\n\\[p(M_n) = (n-1)F(x)^nf(x),\\]\nwhere \\(f(x)\\) is the PDF of the sample distribution."
  },
  {
    "objectID": "posts/2017-06-21-maxvalue.html#moments-with-normal-distrubtions",
    "href": "posts/2017-06-21-maxvalue.html#moments-with-normal-distrubtions",
    "title": "Estimating Max-value Distributions",
    "section": "Moments with Normal distrubtions",
    "text": "Moments with Normal distrubtions\nThere’s really no expression for moments of \\(p(M_n)\\) when \\(n \\gt\\) a few… however, you can use some approximations or bounds. Bounds aren’t that tight… so we’ll use approximations\n\nFirst Moment\nFor large n, there’s an approximate formula from:\n\nAlgorithm AS 177: Expected Normal Order Statistics (Exact and Approximate) Author(s): J. P. Royston Source: Journal of the Royal Statistical Society. Series C (Applied Statistics), Vol. 31, No. 2 (1982), pp. 161-165 Stable URL: http://www.jstor.org/stable/2347982\n\nwhich is\n\\[\\mathbb{E} [M_n] = \\mu + \\Phi^{-1}\\left(\\frac{n-\\alpha}{n-2\\alpha+1}\\right)\\sqrt{\\sigma^2},\\]\nwhere \\(\\Phi^{-1}\\) is the inverse of the normal CDF, and \\(\\alpha\\) is a constant that is \\(\\alpha \\sim 0.375\\). You can numerically optimize \\(\\alpha\\) for the values of \\(n\\) that you are interested in\n\n\n\n\\(\\sqrt{n}\\)\n\\(n\\)\nalpha\n\n\n\n\n3\n9\n0.36205\n\n\n5\n25\n0.37662\n\n\n7\n49\n0.38432\n\n\n9\n81\n0.38897\n\n\n11\n121\n0.39264\n\n\n\n\n\nSecond Moment\nNote that this is going to be the same scaling as the variance, just shifted by \\(\\mu^2\\).\nI couldn’t find anything in the literature that worked well, so I guessed a reasonable form that linearized the dependence of \\(\\mathbb{E}[M_n^2]\\) of Monte Carlo samples upon \\(n\\), \\(\\sigma^2\\), and \\(\\mu\\). I got the following formula, which is NOT perfectly linear, but seems pretty reasonable:\n\\[\\text{Var} = \\mathbb{E}[M_n^2] - \\mathbb{E}[M_n]^2 \\sim \\sigma^2\\cdot \\frac{a+b/n}{\\text{ln}(n)},\\]\nwhere \\(a\\), and \\(b\\) are undetermined constants.\nNow all you have to do is optimize the values of \\(a\\) and \\(b\\), as was done above for \\(\\alpha\\). Note, this isn’t going to be perfect, because the form above isn’t perfect, therefore the values of \\(a\\) and \\(b\\) you get will depend up on the \\(\\mu\\), \\(\\sigma^2\\), and \\(n\\) used to draw the Monte Carlo samples. However, the dependence is pretty minimal. Anyway, I found that the following values work pretty well.\n\n\n\n\\(a\\)\n\\(b\\)\n\n\n\n\n0.85317\n-0.573889"
  },
  {
    "objectID": "posts/2017-06-21-maxvalue.html#estimating-the-underlying-normal-distribution-of-the-samples",
    "href": "posts/2017-06-21-maxvalue.html#estimating-the-underlying-normal-distribution-of-the-samples",
    "title": "Estimating Max-value Distributions",
    "section": "Estimating the underlying Normal distribution of the samples",
    "text": "Estimating the underlying Normal distribution of the samples\nWith the equations above for the approximations to the first and second moments, you can get a pretty decent estimate, by solving for \\(\\sigma^2\\), and then \\(\\mu\\). Here’s some Python code\nimport numpy as np\nfrom scipy import special\n\ndef _estimate_mu(n,var):\n    alpha = 0.375\n    return special.ndtri((n-alpha)/(n-2.*alpha+1.))*np.sqrt(var)\n\ndef _estimate_var(n):\n    a = .85317\n    b = -.573889\n    y = np.log(n)/(a+b/n)\n    return y\n\ndef estimate_from_max(d,n):\n    v = np.var(d)  * _estimate_var(n)\n    m = np.mean(d) - _estimate_mu(n,v)\n    return m,v"
  },
  {
    "objectID": "posts/2023-1124-calibrate.html",
    "href": "posts/2023-1124-calibrate.html",
    "title": "Calibrating an sCMOS camera",
    "section": "",
    "text": "This post is a functional Jupyter notebook that enables the user to calibrate an sCMOS camera. It uses pycromanager and micromanager to collect calibration data, and then Python code to analyze that data and video the calibration parameters. In general, it is equivalent to the approach used by Huang et al. below. The cells are filled with data from a calibration of a Hamamatsu BT Fusion sCMOS camera in fast mode.\nSome useful reading:\n\nThe SI of Huang et al. (2013) Nat. Methods, DOI:10.1038/nmeth.2488\nJanesick, J. R. Photon transfer: DN → \\(\\lambda\\). (SPIE, 2007).\n\n\n\nThis section describes the math behind a camera in familar terms.\nAssume that are incoming photons impinging upon a single detector (i.e., on of the pixels of an sCMOS camera). The rate of those incident photons per measurement is\n\\[ \\lambda = \\lambda^\\prime + \\lambda^{\\prime\\prime} t,\\] where we have denoted a time independent contribution with one prime and a time-dependent contribution with two primes.\nFor any given time period, the probability distribution function (PDF) of the actual number of photons hitting the detector, \\(N\\), will be Poisson-distributed with rate \\(\\lambda\\) such that\n\\[ p(N \\mid \\lambda) = \\text{Poisson}(N\\mid\\lambda) = \\frac{\\lambda^N e^{-\\lambda}}{N!}.\\]\nwithout losing generality, we’ll switch to using electrons instead of photons. A good camera will have a high QE, so most of the incident photon will be converted into photoelectrons, however there are also electrons generated by the camera read process (read noise) and things like (time dependent) dark-currents. So, assuming that \\(N\\) electrons are generated on the camera pixel in question (regardless of their origin…), a camera will return a number in digital units (DU) corresponding to the number of electrons that were detected. The output signal, \\(y\\), is generally offset from 0 by some fixed number \\(y_0\\) to avoid ‘negative’ numbers in unsigned int output of the ADC. This offset addition is a deterministic process, so we can consider the camera to take a number of electrons \\(N\\) and return a number \\(y-y_0\\) in DU. That process is stochastic, and generally follows a normal distribution such that\n\\[ p(y-y_0 \\mid N, \\theta) = \\mathcal{N}(y-y_0 \\mid gN, \\sigma^2), \\]\nwhere \\(g\\) is the gain in \\(DU/e^{-}\\), \\(\\sigma^2\\) is the read noise, and \\(\\theta\\) is a shorthand for the set of model parameters included in the conditional dependency of a statement (e.g., \\(g\\), \\(\\sigma^2\\), and \\(y_0\\) here).\nWhat we’re actually interested in is the PDF of \\(y\\) as a function of \\(\\lambda\\) regardless of the actual number of \\(N\\). To get this, we have to perform a marginalization\n\\[ p(y-y_0 \\mid \\theta) = \\sum_N p(y-y_0\\mid N,\\theta)\\cdot p(N\\mid\\theta). \\]\nThis marginalization calcuation can be performed analytically in the case that \\(N\\) is relatively large (ie, greater than ~1000). In that situation the Poisson distribution can be approximated as a normal distribution, such that\n\\[ \\text{Poisson}(N\\mid\\lambda) \\approx \\mathcal{N}(N\\mid \\lambda,\\lambda), \\] i.e., where the mean and variance of the normal distribution is the rate of the Poisson distribution. This approximation is apparent by moment matching, and more rigorously achieved by comparing the moment generating functions of the Poisson and normal distributions.\nIn that situation, after integrating over N and a lot of rearranging, we find that\n\\[ p(y \\mid \\theta) = \\mathcal{N}\\left(y \\mid g\\lambda^{\\prime\\prime}t + (y_0 + g\\lambda^\\prime), g^2\\lambda^{\\prime\\prime}t + (\\sigma^2 +g^2\\lambda^\\prime)\\right), \\]\nwhich explicity notes the time dependence of these measurements. It’s important to note that while the mean scales as \\(g\\) and the variance scales as \\(g^2\\), both linearly scale with \\(\\lambda^{\\prime\\prime}\\). In short, both the mean and the variance of the output signal of a camera pixel (under relatively high electron counts) are linear in exposure time.\nWhen comparing to the work of Huang et al. in their SI sections one and two, we note that their sCMOS calibration protocol utilizes the same assumptions. Thus, we have the following equivalencies\n\n\n\nHuang et al\nThis work\n\n\n\n\n\\(g_i\\)\n\\(g\\)\n\n\n\\(o_i\\)\n\\((y_0 + g\\lambda^\\prime)\\)\n\n\n\\(var_i\\)\n\\((\\sigma^2 +g^2\\lambda^\\prime)\\)\n\n\n\\(v^k_i - var_i\\)\n\\(g^2 \\lambda^{\\prime\\prime}t\\)\n\n\n\\(\\bar{D^k_i}-o_i\\)\n\\(g\\lambda^{\\prime\\prime}t\\)\n\n\n\nThus these calibrations are completely equivalent, although our parameterizations are little more transparent. However, instead of using their exact protocol, we provide a more convenient calculation to get \\(g\\). Note that if \\(t = 0\\) and/or \\(\\lambda^{\\prime\\prime}=0\\) (i.e., a dark movie), then we expect\n\\[ p(y_{i,\\text{dark}} \\mid \\theta) = \\mathcal{N}(y_{i,\\text{dark}} \\mid o_i, var_i), \\] so we can obtain measurements of \\(y_{i,dark}\\) and use any common approach to get \\(o_i\\) and \\(var_i\\) for each pixel. As discussed below, we use an order-statistics approach to avoid any contamination from salt noise (e.g., cosmic rays hitting the pixels), but the most common would be to use the sample mean and variance to estimate the values of \\(o_i\\) and \\(var_i\\).\nAs Huang et al. note, the estimation for \\(g\\) can be cast as a least-squares minimization problem if we know \\(o_i\\), \\(var_i\\), and have a series of \\(y\\) obtained at several different exposure times and/or illumination intensities. However, their approach requires the use a pseudo-matrix inverse and is kind of annoying to program etc. Instead, here we show the equivalent maximum likelihood estimator of this problem, which is much easier to implement.\nNote that both approaches assume for each \\(k^{th}\\) movie acquired, that\n\\[ \\mathcal{N}(v^k_i-var_i \\mid g(\\bar{D^k_i}-o_i), \\sigma^2_{least squares} ), \\]\nwhere the least squares variance is unimportant, but the same for each dataset. The MLE approach to this problem of estimating g is to take the derivative with respect to g of the log-likelihood (i.e., the product of these normal distributions), set that equal to zero, and solve for g. This gives\n\\[ \\frac{d}{d g_i} \\ln\\mathcal{L}  = 0 =  \\sum_k \\left((v_i^k-var_i) - g(\\bar{D_i^k}-o_i)\\right) \\cdot \\left( -(\\bar{D_i^k}-o_i)\\right) \\] \\[ \\sum_k \\left(v_i^k-var_i \\right)\\left( g(\\bar{D_i^k}-o_i) \\right) = g_i \\sum_k \\left( g(\\bar{D_i^k}-o_i) \\right)^2 \\]\n\\[ g_{i,MLE} = \\frac{ \\sum_k \\left(v_i^k-var_i \\right)\\left( g(\\bar{D_i^k}-o_i) \\right)} { \\sum_k \\left( g(\\bar{D_i^k}-o_i) \\right)^2} \\]\nChecking this with our variables, we see that \\[ g = \\frac{\\sum_k (g^2\\lambda^{\\prime\\prime}t) (g\\lambda^{\\prime\\prime}t)}{\\sum_k (g\\lambda^{\\prime\\prime}t)^2} \\sim \\frac{g^3}{g^2} \\sim g^1.\\]\n\n\n\nThis is a niche subject, but the idea is that if you only look at local extrema, then you can still predict the original distribution. A bit ago, I developed a pretty good approximation to the second-moment for normal distributed variables (read about it here). There are already good approximations to the first-moment in the literature. Thus, the approach we take is to find the local minima (here, at the same pixel, in time), calculate the sample mean and sample variance of those local minima (note, this excludes any salt noise if the local regions are large enough and salt is slow), and then use moment matching to solve for the parameters of the original normal distribution. This works very well."
  },
  {
    "objectID": "posts/2023-1124-calibrate.html#setup",
    "href": "posts/2023-1124-calibrate.html#setup",
    "title": "Calibrating an sCMOS camera",
    "section": "Setup",
    "text": "Setup\nCustomize the variables below for your system\n\nimport numpy as np\n\n## This is a temporary file for each individual movie. Make sure the disk has enough space to hold one movie at a time.\n## It will automatically be deleted.\nfdir = r'C:\\Users\\ptirf\\Desktop'\nfname = r'temp'\n\n## Number of frames to collect -- nb, you really want to have the dark movie dialed in, so use more datapoints there.\nnpoints_light = 1000\nnpoints_dark  = 5000\n\n## Automatically set these imaging parameters at the beginning of each acquisition\nproperties = [\n    ['HamamatsuHam_DCAM','ScanMode',3],\n    ['HamamatsuHam_DCAM','Binning','2x2'],\n]\n\n## Define the exposures to use. nb, often must be given in msec not sec....\nexposure_properties = ['HamamatsuHam_DCAM','Exposure']\nexposures = 10**np.linspace(-4,-1,13) * 1000 ## msec\nprint('Light Exposures (msec):',exposures)\n\ndark_exposure = 0.01 * 1000 ## msec\nprint('Dark Exposure (msec):',dark_exposure)\n\n\n## This script uses min-value order statistics to avoid salt noise. \n## `nskip` is how many frames to look across when finding each minima.\nnskip = 10\n\nLight Exposures (msec): [  0.1          0.17782794   0.31622777   0.56234133   1.\n   1.77827941   3.16227766   5.62341325  10.          17.7827941\n  31.6227766   56.23413252 100.        ]\nDark Exposure (msec): 10.0\n\n\n\nMicroscope Code\n\n\nSoftware\nNote, getting pycromanager working required matching micro-manager and pycromanager version. The trick was to go to pypi (pycromanager is installed using pip), and find out the day when the latest release was released. Then go to the daily builds download of micro-manager and download the daily build from the closest/best matched day.\nFor instance, in November 2023, the latest pycromanager release version was 0.29.9, which was put out on (September 29,2023)[https://pypi.org/project/pycromanager/#history]. The (latest nightly build)[https://download.micro-manager.org/nightly/2.0/Windows/] of micromanger failed, so going back to the 9/29/2023 release worked.\n\n\nApproach\nThis approaches uses min-value order statistics to estimate the gaussian distribution of intensities for each pixel. This approach is to avoid salt noise. It uses (my own moment-matching approximation)[http://ckinzthompson.github.io/posts/2017-06-21-maxvalue.html].\nAlso, it excludes the first frame of every tif file (i.e., if your movie is split across many files, several frames are skipped). This is because it was too much work to figure out which tif file is the actual first of such a series. In that first movie,the first frame seems to be overexposed on my camera and must be skipped. Skipping all the first frames is the quickest solution.\nAlso, each movie is acquired, processed, and then deleted. Processing algorithms keep the movie data as uint16 to save memory. The maximum memory use is when locating the local minima, a copy of size/nskip is made while the original data is around, so that should be 1.1x the file size if using nskip=10. Also note that all tif files contribute to the mean and var (the expectation values of \\(E[x]\\) and \\(E[x^2]\\) are acquired in an online approach, and the mean and var are calculated from them at the end).\n\n\nTiming\nThe processing seems to be the rate-limiting step. Could probably speed up by running pixels in parallel (i.e. nb.prange).\n\nimport numpy as np\nimport numba as nb\nfrom scipy.special import ndtri\n\n@nb.njit\ndef collect_t_mins(d,l):\n    if d.ndim == 2:\n        raise Exception('d should be 3d')\n    nt,nx,ny = d.shape\n    mt = nt//l\n    \n    out = np.zeros((mt,nx,ny),dtype='uint16')\n    for i in range(nx):\n        for j in range(ny):\n            for k in range(mt):\n                out[k,i,j] = np.min(d[k*l:(k+1)*l,i,j])\n    return out\n\n@nb.njit\ndef online_expectations(d):\n    ## the point is the d should stay a uint16 array for memory issues...\n    nt,nx,ny = d.shape\n    out = np.zeros((2,nx,ny),dtype='double')\n    dtij = 0.\n    for t in range(nt):\n        for i in range(nx):\n            for j in range(ny):\n                dtij = float(d[t,i,j])\n                out[0,i,j] += dtij\n                out[1,i,j] += dtij*dtij\n    return out\n                \ndef online_normal_stats(tifs,nskip):\n    '''\n    take a list of tif filenames and return the mean and var per pixel image of all together\n    uses online approx, min-value statistics, and keeps everything uint16 for memory purposes\n    '''\n    #### Calculate the mean and variance of each pixel, including ALL the tif file parts\n    ## Note, there are some improvements to make:\n    ## - the first frame from every file is skipped -- only need to skip it for first file to avoid weird artifacts\n\n    \n    ntifs = len(tifs)\n    ns = np.zeros((ntifs))\n    for i in range(ntifs):\n        ## skip the first time point -- only for movie one but idk which that is now (note, not nec. the first filename in list)\n        tif = tifs[i]\n        d = tifffile.imread(tifs[i])[1:]\n\n        d = collect_t_mins(d,nskip)\n        nt,nx,ny = d.shape\n        if i == 0:\n            expectations = np.zeros((ntifs,2,nx,ny))\n        expectations[i] = online_expectations(d)\n        ns[i] = nt\n    expectations = expectations.sum(0)\n    ns = np.sum(ns)\n    mean = expectations[0]/ns\n    var = expectations[1]/ns - mean**2.\n    \n    ## use min-value statistics to get actual distribution\n    alpha = 0.375\n    factor_mean = ndtri((nskip-alpha)/(nskip-2.*alpha+1.))\n    a = .85317\n    b = -.573889\n    factor_var = np.log(nskip)/(a+b/nskip)\n    \n    tv = var * factor_var\n    tm = mean + factor_mean*np.sqrt(tv)\n    \n    return tm,tv\n\n\nfrom pycromanager import Acquisition, Core, multi_d_acquisition_events\nimport numpy as np\nimport tifffile\nimport shutil\nimport time\nimport os\n\ndef microscope_expose(npoints,exposure,exposure_properties,properties,fdir,fname='temp',nskip=10):\n    ## Setup MMCore\n    core = Core()\n\n    ## Set camera properties\n    for prop in properties:\n        core.set_property(prop[0],prop[1],prop[2])\n\n    ## Set the exposure\n    core.set_property(exposure_properties[0],exposure_properties[1],exposure)\n    time.sleep(.5) ## Make sure it kicks in\n    print('Running', core.get_property(exposure_properties[0],exposure_properties[1]))\n\n    ## Start the acquisition - store it in as .tif files that will be removed \n    ## Originally wanted to do this completely in memory.... bugs?\n    with Acquisition(directory=fdir,name=fname,debug=False,core_log_debug=False,show_display=False) as acq:\n        events = multi_d_acquisition_events(num_time_points=npoints, time_interval_s=0.)\n        acq.acquire(events)\n\n    ## Make sure we close the dataset? \n    dataset = acq.get_dataset()\n    path = dataset.path\n    dataset.close()\n\n    ## Get all the NDTiff files -- large files are split over several individual files\n    tifs = [os.path.join(path, tif) for tif in os.listdir(path) if tif.endswith(\".tif\")]\n\n    ## get statistics\n    print('Calculating',len(tifs),path)\n    mean,var = online_normal_stats(tifs,nskip)\n\n    ## Remove the tif data\n    shutil.rmtree(path)\n    \n    return mean,var"
  },
  {
    "objectID": "posts/2023-1124-calibrate.html#acquire-dark",
    "href": "posts/2023-1124-calibrate.html#acquire-dark",
    "title": "Calibrating an sCMOS camera",
    "section": "Acquire Dark",
    "text": "Acquire Dark\nTurn the light off and make sure absolutely no light makes it to the detector. Probably you should just unhook the camera and put the cap on it… do it. don’t be lazy. Also, plastic caps seem to let light in?\n\n## Collect data, calculate statistics, then remove data\nt0 = time.time()\nmean,var = microscope_expose(npoints_dark,dark_exposure,exposure_properties,properties,fdir,fname,nskip)\nout = np.array([[mean,var],])\n\n## Save the real data\nt1 = time.time()\n\nout = np.array(out)\nnp.save(os.path.join(fdir,'dark_data.npy'),out)\nprint('Done %.3f sec'%(t1-t0))\nprint(out.shape)\n\nRunning 10.0010\nCalculating 4 C:\\Users\\ptirf\\Desktop\\temp_1\\\nDone 185.981 sec\n(1, 2, 1152, 1152)"
  },
  {
    "objectID": "posts/2023-1124-calibrate.html#acquire-light",
    "href": "posts/2023-1124-calibrate.html#acquire-light",
    "title": "Calibrating an sCMOS camera",
    "section": "Acquire Light",
    "text": "Acquire Light\nMake sure to go to the top exposure time, and tune the light source to so that nearly all of the pixels are at the ADC maximum (i.e., \\(2^{16}-1=65535\\)). Also, make sure to remove any image splitters to make sure all pixels are getting a comparable amount of light.\nMy setup: 1. I took a piece of plastic (cut from a petri dish plate) 2. Scratched it up to be a diffuser added a piece of black masking tape b/c it was allowing too much light 3. Placed that over the camera opening 4. Placed kim-wipes on top of that to act as diffusers 5. Placed the mask tape roll on top of the kimwipes to keep them from blowing around 6. Positioned an LED point down onto the camera. 7. Tuned the light source to saturate the camera at the maximum exposure (0.1 sec here…) using the micromanager live preview histogram\n\nout = []\n\ntime.sleep(20) ## for you to get out of the room...\nt0 = time.time()\nfor exposure in exposures:\n    ## Collect data, calculate statistics, then remove data\n    mean,var = microscope_expose(npoints_light,exposure,exposure_properties,properties,fdir,fname,nskip)\n    out.append([mean,var])\n    \n## Save the real data\nt1 = time.time()\nout = np.array(out)\nnp.save(os.path.join(fdir,'light_data.npy'),out)\nnp.save(os.path.join(fdir,'light_exposures.npy'),exposures)\n\nprint('Done %.3f sec'%(t1-t0))\n\nRunning 0.1000\nCalculating 1 C:\\Users\\ptirf\\Desktop\\temp_1\\\nRunning 0.1780\nCalculating 1 C:\\Users\\ptirf\\Desktop\\temp_1\\\nRunning 0.3190\nCalculating 1 C:\\Users\\ptirf\\Desktop\\temp_1\\\nRunning 0.5620\nCalculating 1 C:\\Users\\ptirf\\Desktop\\temp_1\\\nRunning 1.0000\nCalculating 1 C:\\Users\\ptirf\\Desktop\\temp_1\\\nRunning 1.7790\nCalculating 1 C:\\Users\\ptirf\\Desktop\\temp_1\\\nRunning 3.1670\nCalculating 1 C:\\Users\\ptirf\\Desktop\\temp_1\\\nRunning 5.6250\nCalculating 1 C:\\Users\\ptirf\\Desktop\\temp_1\\\nRunning 10.0010\nCalculating 1 C:\\Users\\ptirf\\Desktop\\temp_1\\\nRunning 17.7840\nCalculating 1 C:\\Users\\ptirf\\Desktop\\temp_1\\\nRunning 31.6230\nCalculating 1 C:\\Users\\ptirf\\Desktop\\temp_1\\\nRunning 56.2380\nCalculating 1 C:\\Users\\ptirf\\Desktop\\temp_1\\\nRunning 100.0030\nCalculating 1 C:\\Users\\ptirf\\Desktop\\temp_1\\\nDone 666.046 sec"
  },
  {
    "objectID": "posts/2023-1124-calibrate.html#load-the-data",
    "href": "posts/2023-1124-calibrate.html#load-the-data",
    "title": "Calibrating an sCMOS camera",
    "section": "Load the Data",
    "text": "Load the Data\n\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfdir = './' #fdir\nfname_exposures = os.path.join(fdir,'light_exposures.npy')\nfname_light_data = os.path.join(fdir,'light_data.npy')\nfname_dark_data = os.path.join(fdir,'dark_data.npy')\n\n# ### in case they're non-standard files...\n# fname_exposures  = 'light3_exposures.npy'\n# fname_light_data = 'light3_means_vars.npy'\n# fname_dark_data  = 'dark3_means_vars.npy'\n\nexposures = np.load(fname_exposures)\nlight_data = np.load(fname_light_data)\ndark_data = np.load(fname_dark_data)\n\n\nexposures = exposures/1000. ## they are given in msec"
  },
  {
    "objectID": "posts/2023-1124-calibrate.html#isolate-linear-response-region",
    "href": "posts/2023-1124-calibrate.html#isolate-linear-response-region",
    "title": "Calibrating an sCMOS camera",
    "section": "Isolate linear response region",
    "text": "Isolate linear response region\nFigure out which datapoints don’t look like the expected gaussians – only take the region where increasing exposure time produces a linear change (i.e., below around 1000 electrons you have poisson noise and the normal approximation is bad; above ~60k electrons and many of the pixel ADCs will saturate)\n\n#### These will be excluded\ncut_lower = 1400   ## the 2x2 binning offet is 400, so ...\ncut_upper = 55000  ## the camera has a 16-bit ADC .... so 65535 max\n\n\nmu = light_data[:,0].mean((1,2))\nlast = mu > cut_upper\n\nkeep = np.bitwise_and(mu > cut_lower, mu < cut_upper)\nif np.sum(last) > 0:\n    keep[np.argmax(last):] = False\nnotkeep = np.bitwise_not(keep)\n\nif keep.sum() > 0:\n    plt.loglog(exposures[keep],mu[keep],'o',color='tab:blue')\nif notkeep.sum() > 0:\n    plt.loglog(exposures[notkeep],mu[notkeep],'o',color='tab:red')\n\nplt.axhline(y=65535,color='black')\nplt.axhspan(ymin = 1, ymax = cut_lower,color='tab:red',alpha=.3)\nplt.axhline(y=cut_lower,color='tab:red')\nplt.axhspan(ymin = cut_upper, ymax = 2**16-1,color='tab:red',alpha=.3)\nplt.axhline(y=cut_upper,color='tab:red')\n\nplt.ylim(10**2,10**5)\nplt.ylabel('Avg. Camera (DU)')\nplt.xlabel('Exposure Time (s)')\nplt.show()"
  },
  {
    "objectID": "posts/2023-1124-calibrate.html#calculate-the-calibrations",
    "href": "posts/2023-1124-calibrate.html#calculate-the-calibrations",
    "title": "Calibrating an sCMOS camera",
    "section": "Calculate the calibrations",
    "text": "Calculate the calibrations\nNote, Huang et al use least squares and a matrix based solver that involves pseudo-inverses. To avoid all of that, you can just use the maximum-likelihood estimator (MLE) version of that calculation used here.\nNotes for this camera (Hamamatsu BT Fusion):\n\nIt looks like I need to redo the illumination and/or dark image, because there are rings in the gain image\nThere are weird pixels on this camera at the top (column-wise read out amplifiers?), and on one edge (idk). These can be excluded by slicing [128:,:-128] for the 2x2 binnned images here. Note this brings this camera down to a 1024x1024 or 2048x2048 image. Also, the manual says that running at such a size ROI gets a reasonable speed boost, so maybe I’ll do this is the future.\nThis data is in imaging mode 3 (fast).\n\n\n## These labels follow Huang et al.\no = dark_data[0,0]\nvar = dark_data[0,1]\n\n## they use least squares, here is the MLE version \ng = np.sum((light_data[keep,1] - var[None,:,:])*(light_data[keep,0] - o[None,:,:]),axis=0)/np.sum((light_data[keep,0] - o[None,:,:])**2.,axis=0)\n\nprint(o.shape,var.shape,g.shape)\n\n(1152, 1152) (1152, 1152) (1152, 1152)\n\n\n\nfig,ax = plt.subplots(1,2,figsize=(7,3),dpi=300)\nhy,hx = ax[0].hist(g.flatten(),bins=500,log=True,range=(0,10),color='tab:blue')[:2]\nax[0].hist(g[128:,:-128].flatten(),bins=500,log=True,histtype='step',color='tab:orange')\nax[1].imshow(g,vmin=4,vmax=6)\nax[0].set_xlabel('g (DU/e-)')\nax[0].set_ylabel('Pixels')\nax[1].axis('off')\nfig.tight_layout()\nplt.show()\n\npeak_g = hx[hy.argmax()]\nprint('Maximum:',peak_g)\n\n\n\n\nMaximum: 4.46\n\n\n\nfig,ax = plt.subplots(1,2,figsize=(7,3),dpi=300)\nhy,hx = ax[0].hist(o.flatten(),bins=500,log=True,color='tab:blue')[:2]\nax[0].hist(o[128:,:-128].flatten(),bins=500,log=True,histtype='step',color='tab:orange')\nax[1].imshow(o,vmin=380,vmax=420)\nax[0].set_xlabel('o (DU)')\nax[0].set_ylabel('Pixels')\nax[1].axis('off')\nfig.tight_layout()\nplt.show()\n\npeak_o = hx[hy.argmax()]\nprint('Maximum:',peak_o)\n\n\n\n\nMaximum: 401.9556435675076\n\n\n\nfig,ax = plt.subplots(1,2,figsize=(7,3),dpi=300)\nhy,hx = ax[0].hist(var.flatten(),bins=500,log=True,range=(0,1000),color='tab:blue')[:2]\nax[0].hist(var[128:,:-128].flatten(),bins=500,log=True,histtype='step',color='tab:orange')\nax[1].imshow(var,vmin=50,vmax=400,interpolation='nearest')\nax[0].set_xlabel(r'var ($DU^2$)')\nax[0].set_ylabel('Pixels')\nax[1].axis('off')\nfig.tight_layout()\nplt.show()\n\npeak_var = hx[hy.argmax()]\nprint('Maximum:',peak_var)\nprint('Noisy electrons:',np.sqrt(peak_var)/peak_g)\n\n\n\n\nMaximum: 132.0\nNoisy electrons: 2.5760370612278156\n\n\n\n### Save the calibration\nprint('Saving calibrations (individual and global)')\nprint('g:   %.3f'%(peak_g))\nprint('o:   %.3f'%(peak_o))\nprint('var: %.3f'%(peak_var))\n\nout = np.array([g,o,var,])\nnp.save('calibration_individual.npy',out)\n\nout = np.array([g*0.+peak_g,o*0.+peak_o,var*0+peak_var])\nnp.save('calibration_global.npy',out)\n\nSaving calibrations (individual and global)\ng:   4.460\no:   401.956\nvar: 132.000"
  },
  {
    "objectID": "posts/2023-1124-calibrate.html#analyze-the-calibrations",
    "href": "posts/2023-1124-calibrate.html#analyze-the-calibrations",
    "title": "Calibrating an sCMOS camera",
    "section": "Analyze the calibrations",
    "text": "Analyze the calibrations\n\nfig,ax = plt.subplots(1,2,figsize=(7,3))\n\navgphotons = (light_data[:,0]-o)/g\nlam = np.median((np.median(avgphotons,axis=(1,2))/exposures)[:-1])\nax[0].loglog(exposures[:-1],(np.mean(avgphotons,axis=(1,2))/exposures)[:-1],'o',color='tab:blue',label='Data')\nax[0].axhline(y=lam,color='tab:orange',label='Median')\nax[0].set_xlabel('Exposure (s)')\nax[0].set_ylabel('Avg. Photon rate (s^-1)')\nax[0].legend()\n\nt = 10**np.linspace(np.log10(exposures[0])//1-1,np.log10(exposures[-1])//1+1,1000)\nyy = g.mean()*lam*t+o.mean()\nyy[yy>=2**16-1] = 2**16-1\nax[1].loglog(t,yy,label='Model',color='tab:orange')\nax[1].loglog(exposures,light_data[:,0].mean((1,2)),'o',label='Data',color='tab:blue')\nax[1].legend()\n\nax[1].axhline(y=cut_lower,color='tab:red')\nax[1].axhline(y=cut_upper,color='tab:red')\n\nax[1].set_ylabel('Avg. Camera Counts (DU)')\nax[1].set_xlabel('Exposure (s)')\nfig.tight_layout()\nplt.show()\n\n\n\n\n\nNotes\nThe Hamamatsu BT Fusion documations quotes read noise levels as:\n\nmode 3 (fast): 1.6 RMS e-\nmode 2 (standard): 1.0 RMS e-\nmode 1 (ultra-quiet): 0.7 RMS e-"
  },
  {
    "objectID": "posts/2017-06-23-autocorrelation.html",
    "href": "posts/2017-06-23-autocorrelation.html",
    "title": "Autocorrelation Function Precision",
    "section": "",
    "text": "Following Berne and Pecora’s Dynamic Light Scattering with Applications to Chemistry, Biology, and Physics, the autocorrelation function of a time-dependent signal, \\(A(t)\\), is defined by\n\\[\\begin{aligned}\n\\langle A(0)A(\\tau) \\rangle &= \\lim_{T \\to \\infty} \\frac{1}{T} \\int_0^T dt\\, A(t)A(t+\\tau) \\\\\n&\\cong \\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{j=1}^N A_j A_{j+n},\n\\end{aligned}\\]\nwhere the bottom line is for a signal sampled at discrete intervals \\(\\Delta t\\) such that \\(t = j\\Delta t\\), and \\(\\tau = N\\Delta t\\).\n\n\n\nAt each point, the autocorrelation function is an average. The average is of the product of the signal and the signal after a given delay, \\(\\tau\\). Therefore, what we’re really interested in is the average value of a set of these products. Given a finite-length vector, you will only have a limited number of these products at each \\(\\tau\\). In particular, at large values of \\(\\tau\\), you have very few products contributing to the average. In fact, at the maximum \\(\\tau\\) you only have one product contributing to the average. Therefore the precision with which you can define that actually value of the autocorrelation function given only a few of these products is limited. But, we can leverage Bayesian inference to help us out and quantify that precision."
  },
  {
    "objectID": "posts/2017-06-23-autocorrelation.html#bayesian-inference",
    "href": "posts/2017-06-23-autocorrelation.html#bayesian-inference",
    "title": "Autocorrelation Function Precision",
    "section": "Bayesian Inference",
    "text": "Bayesian Inference\nWe will assume that the products at a particular value of \\(\\tau\\) mentioned above are distributed according to a normal distribution with a mean, \\(\\mu\\), and a precision, \\(\\lambda = 1/\\sigma^2\\), that are unknown. Given a set of these samples at the particular value of \\(\\tau\\), we can use Bayesian inference to infer the value of \\(\\mu\\), which corresponds to the value of the autocorrelation function at that \\(\\tau\\).\n\nPrior\nWe can do this inference easily with a normal-gamma distribution conjugate prior, i.e.,\n\\[\\begin{aligned}p(\\mu,\\lambda \\vert D) &= NG(\\mu,\\lambda \\vert \\mu_0, \\kappa_0,\\alpha_0,\\beta_0) \\\\\n&= \\frac{1}{Z_0} \\lambda^{\\alpha_0-0.5} \\cdot e^{-\\frac{\\lambda}{2}\\left(\\kappa_0(\\mu-\\mu_0)^2+2\\beta \\right)}\n\\end{aligned}\\]\nwhere\n\\[Z_0 =  \\frac{\\Gamma(\\alpha)}{\\beta^\\alpha}\\left(\\frac{2\\pi}{\\kappa_0}\\right)^{0.5},\\]\nand where \\(\\Gamma(x)\\) is the gamma function.\n\n\nPosterior\nFollowing Kevin Murphy’s nice paper, Conjugate Bayesian analysis of the Gaussian distribution, we can easily evaluate the posterior probability distribution of a normal likelihood, and a normal-gamma conjugate prior. The posterior probability distribution is also a normal-gamma distribution with\n\\[\\begin{aligned}\n\\kappa_n &= \\kappa_0 + n \\\\\n\\alpha_n &= \\alpha_0 + n/2 \\\\\n\\mu_n &= \\mu_0 \\frac{\\kappa_0\\mu_0+n\\bar{x}}{\\kappa_n}\\\\\n\\beta_n &= \\beta_0 + \\frac{1}{2}\\sum_{i=1}^n (x_i - \\bar{x})^2 + \\frac{\\kappa_0 n (\\bar{x} - \\mu_0)^2}{2\\kappa_n},\n\\end{aligned}\\]\nwhere \\(n\\) is the number of datapoints in the set of products at the particular \\(\\tau\\) being evaluated, and \\(\\bar{x}\\) is the mean of those products in that set.\n\n\nPosterior Marginals\nActually, if we’re just interested in the autocorrelation function, we dont’ really care about \\(\\lambda\\), so we can marginalize it out of the posterior distribution given above. The posterior marginal for \\(\\mu\\) alone given the data, \\(\\{A(t_i)A(t_i+\\tau)\\}\\), is a Student’s T distribution\n\\[p(\\mu \\vert \\mu_n, \\kappa_n, \\alpha_n, \\beta_n, \\{A(t_i)A(t_i+\\tau)\\}) = T_{2 \\alpha_n} \\left( \\mu \\vert \\mu_n, \\frac{\\beta_n}{\\alpha_n\\kappa_n}\\right),\\]\nwhere the second argument is the variance parameter (\\(\\sigma^2\\)). Note that the cumulative distribution function (CDF) of the Student’s T distribution can be used to calculate the credible intervals on \\(\\mu\\) given this marginalized posterior probability distribution."
  },
  {
    "objectID": "posts/2017-06-23-autocorrelation.html#precision-of-the-autocorrelation-function",
    "href": "posts/2017-06-23-autocorrelation.html#precision-of-the-autocorrelation-function",
    "title": "Autocorrelation Function Precision",
    "section": "Precision of the Autocorrelation function",
    "text": "Precision of the Autocorrelation function\nWe can calculate the autocorrelation function as shown above, but this does not give any notion of the precision of the autocorrelation function (particularly at large \\(\\tau\\)). Instead, we will use Bayesian inference to infer the posterior probability distribution describing the autocorrelation function at every delay. This will provide us with the mean value, and a credible interval - the latter of which provides an idea of the precision with with the autocorrelation function is defined at any given \\(\\tau\\). The credible interval (95%) blows up at large \\(\\tau\\) (i.e., n), showing us to be wary about using the mean value. Note, that the autocorrelation function is normalized to the first datapoint, \\(\\tau=0\\)."
  },
  {
    "objectID": "posts/2017-06-22-ts_energy_dists.html",
    "href": "posts/2017-06-22-ts_energy_dists.html",
    "title": "Distributions of Transition State Energies",
    "section": "",
    "text": "Let’s say that you’re analyzing the survival probability (i.e., \\(1-CDF\\)) of the lifetime, \\(t\\), of a molecule in a particular state with rate constant, \\(k\\). If the system is Markovian, then this probability is\n\\[S(t\\vert k) = e^{-k*t}\\]\nbut, what if there is a distribution of rate constants? What does this look like? Following Austin … Frauenfelder, Biochemistry 1975, they note that they see power-law decays for the probably of ligand rebinding to myoglobin, and that this is probably because of a distribution of activation energies for the barrier crossings. They give an expression that looks quite a bit like the gamma distribution, so let’s start there. What if you have rate constants distributed according to the gamma distribution,\n\\[G(k\\vert \\alpha , \\beta) = \\frac{\\beta^\\alpha k^{\\alpha-1} e^{-\\beta k}}{\\Gamma (\\alpha )}\\]. We can then marginalize \\(k\\) out of the survival probability expression as follows\n\\[\\begin{aligned}\nS(t \\vert \\alpha, \\beta) &= \\int_0^\\infty dk \\cdot S(t \\vert k) \\cdot G(k \\vert \\alpha, \\beta) \\\\\n&= \\frac{\\beta^\\alpha}{\\Gamma (\\alpha )} \\int_0^\\infty dk \\cdot k^{\\alpha - 1}e^{-k(t + \\beta)} \\\\\n&= \\frac{\\beta^\\alpha}{\\Gamma (\\alpha )} \\int_0^\\infty \\frac{dz}{t + \\beta} \\cdot \\left(\\frac{z}{t + \\beta}\\right)^{\\alpha - 1}e^{-z},\\text{ where } z \\equiv (t+\\beta)k \\\\\n&= \\frac{\\beta^\\alpha}{\\Gamma (\\alpha )} (t+\\beta )^{-\\alpha} \\int_0^\\infty dz \\cdot z^{\\alpha-1}\\cdot e^{-z} \\\\\n&= \\frac{\\beta^\\alpha}{\\Gamma (\\alpha )} (t+\\beta )^{-\\alpha} \\cdot \\Gamma (\\alpha) \\\\\n&= \\left( \\frac{\\beta}{t + \\beta}\\right)^\\alpha\n\\end{aligned}\\]\nThis is pretty interesting, because the form is actually a power-law decay, just like Austin et al said. Anyway, going with the gamma distribution interpretation of these parameters, you find the \\(\\beta\\) is the (constant) time of each sub-step (i.e., elementary reactions) involved in the decay step out of the molecular state. Additionally, \\(\\alpha\\) is the number of sub-steps made be decay step (i.e., this is an Erlang distribution interpretation). Austin et al give an expression for this distribution,\n\\[g(k) = \\frac{(kt)^n e^{-kt}}{RT \\Gamma (n)}.\\]\nTo make sure that this distribution is normalized(!!), we can match the parameters given here with the parameters of the gamma distribution. If we find that \\(\\alpha \\sim n\\), \\(\\beta \\sim t\\), and \\(k \\sim k\\). Thus, those parameters have the same interpretation. To ensure that \\(g(k)\\) is normalized, we find that \\(\\alpha = RT\\). This is an interesting thought. For instance, is this a result of the equipartition of energy? I guess it means that the transition involves more sub-steps with increasing temperature (linearly). That is kind of a strange thought; \\(\\beta\\) should also have some temperature dependence (i.e. it should decrease with increasing temperature)."
  },
  {
    "objectID": "posts/2017-06-22-ts_energy_dists.html#normal",
    "href": "posts/2017-06-22-ts_energy_dists.html#normal",
    "title": "Distributions of Transition State Energies",
    "section": "Normal",
    "text": "Normal\nA less exact case is if the rate constants are distributed according to a normal distribution, \\(\\mathcal{N}(k \\vert \\mu, \\sigma)\\). This is a bit of a stretch, because rate constants aren’t negative, but the support of the normal distribution is from \\(-\\infty\\) to \\(+\\infty\\). However, as long as the normal distribution is very small when \\(k < 0\\), then this is fairly reasonable. Here, we’ll ignore the bounds of the integral for that reason. An additional note is that the gamma distribution should begin to approximate the normal distribution when \\(\\alpha\\) becomes quite large. Anyway, if we marginalize out \\(k\\), as above, we find that\n\\[\\begin{aligned}\nS(k \\vert \\mu, \\sigma) &= \\int dk \\cdot \\mathcal{N} (k \\vert \\mu, \\sigma)\\cdot e^{-kt} \\\\\n&= \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\int dk \\cdot e^{\\frac{-1}{2\\sigma^2} (k-\\mu)^2 - kt}\\\\\n&= \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\int dk \\cdot e^{\\frac{-1}{2\\sigma^2} \\left(k^2 - 2\\mu k + \\mu^2 - 2\\sigma^2 kt \\right)}\\\\\n&= \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\int dk \\cdot e^{\\frac{-1}{2\\sigma^2} \\left( (k-(\\mu-\\sigma^2t))^2 + 2\\mu \\sigma^2 t - \\sigma^4 t^2 \\right)}\\\\\n&= \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\int dk \\cdot e^{\\frac{-1}{2\\sigma^2} \\cdot (k-(\\mu-\\sigma^2t))^2}e^{\\frac{-1}{2\\sigma^2} ( 2\\mu \\sigma^2 t - \\sigma^4 t^2 )}\\\\\n&= e^{\\frac{-1}{2\\sigma^2} ( 2\\mu \\sigma^2 t - \\sigma^4 t^2 )}\\cdot \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\int dk \\cdot e^{\\frac{-1}{2\\sigma^2} \\cdot (k-(\\mu-\\sigma^2t))^2}\\\\\n&= e^{\\frac{-1}{2\\sigma^2} ( 2\\mu \\sigma^2 t - \\sigma^4 t^2 )}\\cdot \\int dk \\cdot \\mathcal{N} ( k\\vert \\mu-\\sigma^2 t, \\sigma^2 )\\\\\n&= e^{\\frac{-1}{2\\sigma^2} ( 2\\mu \\sigma^2 t - \\sigma^4 t^2 )} \\\\\n&= e^{-\\mu t + \\frac{1}{2} \\sigma^2 t^2}.\n\\end{aligned}\\]\nThis expression is exact if there is not a significant contribution from the \\(k < 0\\) region (i.e., very slow rates, where the decay times are very long). If there are those effects, you will find that at small \\(t\\), the form is exact, and then at long \\(t\\), you see a curved deviation away from the exact solution towards increasing survival probability. That’s obviously wrong, but it’s the effects of the \\(k < 0\\) region."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Single Molecule Biophysics",
    "section": "",
    "text": "The Kinz-Thompson lab is in the Chemistry Department at Rutgers Newark"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "2020 - Current\n\nGentry, R.C., Ide, N.A., Comunale, V.M., Hartwick, E.W., Kinz-Thompson, C.D., Gonzalez Jr., R.L. (2023) The mechanism of mRNA activation. bioRxiv, DOI:10.1101/2023.11.15.567265. [Preprint]\nRay, K. K. & Kinz-Thompson, C. D. (2023) Scale-dependent hierarchical resolution: applications to atomic resolution and model validation in cryoEM. bioRxiv, DOI:10.1101/2023.10.17.562748. [Preprint] [Zenodo Dataset]\nVerma, A. R., Ray, K. K., Bodick, M., Kinz-Thompson, C. D. & Gonzalez, Jr., R. L. (2023) Increasing the accuracy of single-molecule data analysis using tMAVEN. bioRxiv. DOI:10.1101/2023.08.15.553409. [Preprint]\nRay, K.K., Kinz-Thompson, C.D., Fei, J., Wang, B., Lin, Q., Gonzalez, Jr., R.L. (2023) Entropic control of the free energy landscape of an archetypal biomolecular machine. Proceedings of the National Academy of Sciences 120, e2220591120. [Link] [Preprint]\nZhu, L., Kim, J., Leng, K., Ramos, J.E., Kinz-Thompson, C.D., Karpowich, N.K., Gonzalez Jr., R.L., Hunt, J.F. (2022) Realtime observation of ATP-driven single B12 molecule translocation through BtuCD-F. bioRxiv. DOI:10.1101/2022.12.02.518935. [Preprint]\nZhu, L., Kim, J., Leng, K., Ramos, J.E., Kinz-Thompson, C.D., Karpowich, N.K., Gonzalez Jr., R.L., Hunt, J.F. (2022) Mechanistic implications of the interaction of the soluble substrate-binding protein with a type II ABC importer, bioRxiv. DOI:10.1101/2022.12.02.518933. [Preprint]\nKinz-Thompson, C.D., Lopez Redondo, M., Mulligan C., Sauer, D.B., Marden, J.J., Song, J. Tajkhorshid, E., Hunt, J.F., Stokes, D.L., Mindell, J.A., Wang, D.N, Gonzalez, Jr., R.L. (2022) Elevator mechanism dynamics in a sodium-coupled dicarboxylate transporter, bioRxiv. DOI:10.1101/2022.05.01.490196. [Preprint]\nRay, K.K., Verma, A.R., Gonzalez, Jr., R.L., Kinz-Thompson, C.D. (2022) Inferring the shape of data: a probabilistic framework for analysing experiments in the natural sciences. Proceedings of the Royal Society A, 478, 20220177. DOI:10.1098/rspa.2022.0177. [Link] [Preprint]\nKinz-Thompson, C.D., Ray, K.K., Gonzalez, Jr., R.L. (2021) Bayesian inference: the comprehensive approach to analyzing single-molecule experiments. Annual Review of Biophysics, 50, 191-208. DOI:10.1146/annurev-biophys-082120-103921. [Link] [PMC] [Preprint]\n\n\n\nPrior to 2020\n\nSubramanyam, S., Kinz-Thompson, C.D., Gonzalez, R.L., Jr., Spies, M. (2018) Observation and Analysis of RAD51 Nucleation Dynamics at Single-monomer Resolution. Methods in Enzymology, 600, 201-232. [Link] [PMC]\nKinz-Thompson, C.D.,† Gonzalez, R.L., Jr. (2018) Increasing the Time Resolution of Single-molecule Experiments with Bayesian Inference. Biophysical Journal, 114, 289-300. [Link] [PMC] [Preprint]\nKinz-Thompson, C.D., Bailey, N.A., Gonzalez, R.L., Jr. (2016) Precisely and Accurately Inferring Single-molecule Rate Constants. Methods in Enzymology, 581, 187-225. [Link] [PMC]\nKinz-Thompson, C.D., Sharma, A.K., Frank, J., Gonzalez, R.L., Jr. Chowdhury, D. (2015) Quantitative Connection between Ensemble Thermodynamics and Single-Molecule Kinetics: A Case Study Using Cryogenic Electron Microscopy and Single-Molecule Fluorescence Resonance Energy Transfer Investigations of the Ribosome. Journal of Physical Chemistry B, 119(34), 10888-10901. [Link] [PMC]\nKinz-Thompson, C.D., Gonzalez, R.L., Jr. (2014) smFRET Studies of the “Encounter” Complexes and Subsequent Intermediate States that Regulate the Selectivity of Ligand Binding. FEBS Letters, 588(19), 3526-3538. [Link] [PMC]\nKinz-Thompson, C.D., Palma, M., Pulukkunat, D.K., Chenet, D., Hone, J., Wind, S.J., Gonzalez, R.L., Jr. (2013) Robustly Passivated, Gold Nanoaperture Arrays for Single-Molecule Fluorescence Microscopy. ACS Nano, 7(9), 8159-8166. [Link] [PMC]\nKravec, S.M., Kinz-Thompson, C.D., Conwell, E.M. (2011) Localization of a Hole on an Adenine-Thymine Radical Cation Embedded in B-Form DNA in Water. Journal of Physical Chemistry B, 115(19), 6166-6171. [Link]\nKinz-Thompson, C., Conwell, E. (2010) Proton Transfer in Adenine-Thymine Radical Cation Embedded in B-Form DNA. Journal of Physical Chemistry Letters, 1(9), 1403-1407. [Link]\nKucherov, V.M., Kinz-Thompson, C.D., Conwell, E.M. (2010) Polarons in DNA Oligomers. Journal of Physical Chemistry C, 114(3), 1663-1666. [Link]"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software",
    "section": "",
    "text": "We’re committed to free and open source software. Primarily, we use Python/Arduino/C to develop solutions to our experimental and analysis problems. Interested in collaborating? Please, get in touch!\n\nHARP\n\n\n\n\n\n\n\nHierarchical Atomic Resolution Perception (HARP) is software for assessing cryoEM model quality.\n\n\n\n\nRead the preprint: Scale-dependent hierarchical resolution: applications to atomic resolution and model validation in cryoEM\nVisit the HARP website\nSee the code on GitHub\n\n\n\n\ntMAVEN\n\n\n\n\n\n\n\ntime-series Modeling, Analysis, and Visualization ENvironment (tMAVEN) is software for post-processing, modeling, and generating publication quality figures for smFRET data. It was made in collaboration with the Gonzalez lab.\n\n\n\n\nRead the preprint: Increasing the accuracy of single-molecule data analysis using tMAVEN\nVisit the tMAVEN website\nSee the code on GitHub\n\n\n\n\nBIASD\n\n\n\n\n\n\n\nBayesian Inference for the Analysis of Sub-temporal-resolution Data (BIASD) is a method to temporally resolve the kinetics a single-molecule time series, even if those kintics are orders of magnitude faster than the time resolution of the technique.\n\n\n\n\nRead the first paper: Increasing the Time Resolution of Single-Molecule Experiments with Bayesian Inference\nRead the next paper: Entropic control of the free-energy landscape of an archetypal biomolecular machine\nSee the code on GitHub\n\n\n\n\nvbscope\n\n\n\n\n\n\n\nvbscope is archived software from the Gonzalez lab. for analyzing single-molecule localization microscopy data.\n\n\n\n\nSee the code on GitHub"
  },
  {
    "objectID": "people.html",
    "href": "people.html",
    "title": "People",
    "section": "",
    "text": "Meet the Labbies and Crabbies\n\nPrincipal Investigator\n\n\n\n\n\n\nProf. Colin Kinz-Thompson\n \n\n\n\n\n\nResearch Scientists\n\n\n\n\n\n\nDr. Natalia Nemeria\nResearch Associate\n  \n\n\n\n\n\nGraduate Students\n\n\n\n\n\n\nKatherine Leon Hernandez\nSREB Doctoral Scholar, NIH MBRS Scholar, NSF LSAMP Scholar\n  \n\n\n\n\n\n\n\n\n\nAndres Cifuentes\n  \n\n\n\n\n\n\n\n\n\nTianyue Dai\n  \n\n\n\n\n\n\n\n\n\nZhutao Sheng\n  \n\n\n\n\n\n\n\n\n\nVaishnavi Shesham\n  \n\n\n\n\n\nUndergraduate Researchers\n\n\n\n\n\n\nKarl Gaiser\n  \n\n\n\n\n\nPast Members\n\nGraduate Students\n\nYuntao Qiu, Spring 2021 - Summer 2022, yq123@scarletmail.rutgers.edu\n\n\n\nUndergraduate Students\n\nKaiyang Zhu, Summer 2023, kaiyang.zhu@emory.edu\nWesley Chan, Spring 2022 - Summer 2022, wc518@scarletmail.rutgers.edu\n\n\n\nHigh School Students\n\nSasche Joseph, Summer 2022, ACS Project SEED Student\n\n\n\n\nPets\n\n\n\n\n\n\nOnyx\nLab(rador retriever)\n\n\n\n\n\n\n\n\n\nCilantra\nCat(ion exchange chromatography resin)"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Spectroscopy\n\nSpectra Viewer\nR0 Calculator\nRay Optics Simulation\n\n\n\nMolecular Biology\n\nBarrick Lab Protocols\nAddgene: Intro to Lab Bench\n\n\n\nBiomolecular Structure\n\nRCSB Protein Data Bank\nPDBx-mmCIF Syntax\n\n\n\nMath\n\nConjugate Priors - Table\nLaplace Transform - Properties\nLaplace Transform - Table\nFourier Transform - Table\nGamma Function\nIntegrals of exponential functions\n\n\n\nChemistry\n\nNot Voodoo - Demystifies Synthetic Organic Chemistry\nSigma - Reagent Grade Table\n\n\n\nProfessional\n\nSciENcv - NIH/NSF biosketches\nDMPTool - Build your data management plan"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Posts",
    "section": "",
    "text": "Here are some interesting posts about molecules, math, programming, etc.\n\n\n\n\n\n\n\n\n\n\nCalibrating an sCMOS camera\n\n\n\n\n\nA working, python-based calibration protocol for sCMOS cameras\n\n\n\n\n\n\nNov 24, 2023\n\n\nColin Kinz-Thompson\n\n\n\n\n\n\n\n\nFast Median Filters\n\n\n\n\n\nA quick investigation of how to speed up median filtering of 2D images\n\n\n\n\n\n\nOct 29, 2023\n\n\nColin Kinz-Thompson\n\n\n\n\n\n\n\n\nAutocorrelation Function Precision\n\n\n\n\n\nA Bayesian take on estimating ACFs\n\n\n\n\n\n\nJun 23, 2017\n\n\nColin Kinz-Thompson\n\n\n\n\n\n\n\n\nDistributions of Transition State Energies\n\n\n\n\n\nFrauenfelder-inspired distributions\n\n\n\n\n\n\nJun 22, 2017\n\n\nColin Kinz-Thompson\n\n\n\n\n\n\n\n\nEstimating Max-value Distributions\n\n\n\n\n\nA decent approximation for order statistics\n\n\n\n\n\n\nJun 21, 2017\n\n\nColin Kinz-Thompson\n\n\n\n\n\n\nNo matching items"
  }
]